<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large Language Models on RJChow&#39;s Notes</title>
    <link>http://localhost:1313/tags/large-language-models/</link>
    <description>Recent content in Large Language Models on RJChow&#39;s Notes</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>RJChow</copyright>
    <lastBuildDate>Thu, 24 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/large-language-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Explainer</title>
      <link>http://localhost:1313/posts/llm-explainer/</link>
      <pubDate>Thu, 24 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/llm-explainer/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s been quite a few articles that&amp;rsquo;s been written about AI all over the internet, with various technical depths and contexts. But I find that there&amp;rsquo;s a opportunity here to provide an explainer without specialist ML/AI/Statistics background knowledge and jargon. The aim of this article is to provide a background on Large Language Model (LLM) progression, and a more informed perspective on how LLMs work and their limitations.&lt;/p&gt;&#xA;&lt;p&gt;Disclaimer: this article consists of ~95% organic, free-range, fair trade, human-written words. It is not original research but rather secondary research with the aid of LLMs and search engines.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
